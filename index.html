<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" />
<title>Voice Emotion Detector</title>
<style>
  /* Reset */
  * {
    box-sizing: border-box;
  }
  body {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    margin: 0;
    background: linear-gradient(135deg, #667eea, #764ba2);
    color: #fff;
    min-height: 100vh;
    display: flex;
    flex-direction: column;
    align-items: center;
    justify-content: flex-start;
    padding: 1rem;
  }
  header {
    text-align: center;
    margin-bottom: 1rem;
  }
  header h1 {
    font-weight: 900;
    font-size: 2rem;
    margin-bottom: 0.25rem;
  }
  header p {
    font-size: 1rem;
    opacity: 0.85;
    margin-top: 0;
  }
  main {
    background: rgba(255 255 255 / 0.1);
    border-radius: 12px;
    padding: 1rem 1.5rem 2rem 1.5rem;
    max-width: 400px;
    width: 100%;
    box-shadow: 0 8px 30px rgba(0,0,0,0.3);
    display: flex;
    flex-direction: column;
  }
  label {
    font-weight: 600;
    margin-bottom: 0.25rem;
  }
  input[type="file"] {
    width: 100%;
    margin-bottom: 1rem;
    border-radius: 8px;
    padding: 0.5rem;
    font-size: 0.9rem;
  }
  .record-btn {
    background: #764ba2;
    border: none;
    color: #fff;
    font-weight: 700;
    cursor: pointer;
    border-radius: 50px;
    padding: 0.5rem 1.5rem;
    font-size: 1rem;
    transition: background 0.3s ease;
    margin-bottom: 1rem;
    user-select: none;
  }
  .record-btn.recording {
    background: #e65858;
    animation: pulse 1.2s infinite alternate;
  }
  @keyframes pulse {
    0% { box-shadow: 0 0 0 0 rgba(230, 88, 88, 0.7); }
    100% { box-shadow: 0 0 10px 8px rgba(230, 88, 88, 0); }
  }
  textarea {
    width: 100%;
    resize: vertical;
    min-height: 100px;
    border-radius: 8px;
    border: none;
    padding: 0.75rem;
    font-size: 1rem;
    font-family: inherit;
    margin-bottom: 1rem;
  }
  .result-section {
    background: rgba(0,0,0,0.2);
    border-radius: 12px;
    padding: 1rem;
    margin-top: 0.5rem;
  }
  .emotion {
    font-weight: 700;
    font-size: 1.25rem;
    margin-bottom: 0.5rem;
    text-transform: capitalize;
  }
  .tips {
    font-size: 1rem;
    line-height: 1.3;
    margin-top: 0;
  }
  .warning {
    background: #ffc107;
    color: #333;
    padding: 0.5rem 1rem;
    border-radius: 8px;
    font-weight: 600;
    margin-bottom: 1rem;
  }
  footer {
    margin-top: auto;
    font-size: 0.75rem;
    opacity: 0.6;
    text-align: center;
    padding: 1rem 0 0 0;
    width: 100%;
  }
  /* Responsive */
  @media (max-width: 400px) {
    main {
      padding: 1rem;
      max-width: 100%;
    }
  }
</style>
</head>
<body>
<header>
  <h1>Voice Emotion Detector</h1>
  <p>Upload a recording or record live to detect emotions & get tips</p>
</header>
<main>
  <label for="audioUpload">Upload Voice Recording (wav/mp3/m4a):</label>
  <input type="file" id="audioUpload" accept="audio/*" />

  <label for="languageSelect">Select Language:</label>
  <select id="languageSelect" style="margin-bottom: 1rem; border-radius: 8px; padding: 0.5rem; font-size: 1rem; width: 100%;">
    <option value="en-IN" selected>English (India)</option>
    <option value="hi-IN">Hindi</option>
    <option value="ja-JP">Japanese</option>
    <option value="ko-KR">Korean</option>
    <option value="zh-CN">Chinese (Mandarin)</option>
    <option value="ta-IN">Tamil</option>
    <option value="te-IN">Telugu</option>
    <option value="bn-IN">Bengali</option>
    <option value="mr-IN">Marathi</option>
    <option value="gu-IN">Gujarati</option>
    <option value="pa-IN">Punjabi</option>
    <option value="ml-IN">Malayalam</option>
    <option value="or-IN">Odia</option>
    <option value="as-IN">Assamese</option>
  </select>

  <button id="recordButton" class="record-btn" title="Click to start recording">Start Recording</button>

  <label for="transcript">Transcribed Text:</label>
  <textarea id="transcript" rows="6" placeholder="Transcription will appear here..." readonly></textarea>

  <div id="emotionResult" class="result-section" style="display:none;">
    <div class="emotion" id="detectedEmotion"></div>
    <p class="tips" id="emotionTips"></p>
    <label for="llmResponse">Empathetic Response:</label>
    <textarea id="llmResponse" rows="4" readonly style="width: 100%; border-radius: 8px; padding: 0.5rem; font-family: inherit; margin-top: 0.5rem;"></textarea>
    <div id="weightedEmotionScores" style="margin-top: 0.5rem; font-weight: 600;"></div>
    <canvas id="weightedEmotionChart" style="margin-top: 1rem; max-width: 100%; height: 200px;"></canvas>
  </div>

  <div id="warningMsg" class="warning" style="display:none;"></div>
</main>
<footer>
  &copy; 2024 Voice Emotion Detector. Uses Web Speech API & client-side sentiment analysis.
</footer>

<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
<script>
  // Sentiment analysis library (simple polarity based)
  class Sentiment {
    constructor() {
      this.positiveWords = ["happy", "joy", "love", "excited", "great", "wonderful", "fantastic", "good", "pleased", "delight"];
      this.negativeWords = ["sad", "anger", "angry", "upset", "bad", "hate", "terrible", "worst", "frustrated", "depressed"];
      this.sadWords = ["sad", "depressed", "heartbroken", "lonely", "cry", "tears", "melancholy"];
      this.angryWords = ["angry", "frustrated", "mad", "furious", "irritated", "annoyed"];
      this.happyWords = ["happy", "joy", "pleased", "excited", "delight", "smile", "cheerful"];
      this.fearWords = ["fear", "scared", "nervous", "anxious", "worried"];
      this.surpriseWords = ["surprise", "amazed", "astonished", "shocked"];
    }
    analyze(text) {
      const textLower = text.toLowerCase();
      let score = 0;
      let emotionCount = {happy:0, sad:0, angry:0, fear:0, surprise:0, neutral:0};
      
      // Simple count how many words match each emotion category
      this.happyWords.forEach(w => { if(textLower.includes(w)) emotionCount.happy++; });
      this.sadWords.forEach(w => { if(textLower.includes(w)) emotionCount.sad++; });
      this.angryWords.forEach(w => { if(textLower.includes(w)) emotionCount.angry++; });
      this.fearWords.forEach(w => { if(textLower.includes(w)) emotionCount.fear++; });
      this.surpriseWords.forEach(w => { if(textLower.includes(w)) emotionCount.surprise++; });

      // Determine dominant emotion or neutral
      let dominantEmotion = "neutral";
      let maxCount = 0;
      for(const [emotion, count] of Object.entries(emotionCount)) {
        if(count > maxCount) {
          maxCount = count;
          dominantEmotion = emotion;
        }
      }
      return dominantEmotion;
    }
  }

  const sentiment = new Sentiment();

  const recordButton = document.getElementById('recordButton');
  const transcriptArea = document.getElementById('transcript');
  const audioUpload = document.getElementById('audioUpload');
  const emotionResultSection = document.getElementById('emotionResult');
  const detectedEmotionElem = document.getElementById('detectedEmotion');
  const emotionTipsElem = document.getElementById('emotionTips');
  const warningMsg = document.getElementById('warningMsg');

  let mediaRecorder;
  let audioChunks = [];
  let isRecording = false;

  function displayEmotion(emotion, llmResponse, weightedEmotions) {
    // Determine dominant emotion from weightedEmotions if available
    if (weightedEmotions && Object.keys(weightedEmotions).length > 0) {
      let dominantEmotion = "neutral";
      let maxScore = -Infinity;
      for (const [emo, score] of Object.entries(weightedEmotions)) {
        if (score > maxScore) {
          maxScore = score;
          dominantEmotion = emo;
        }
      }
      emotion = dominantEmotion;
    }

    // Show detected emotion text
    detectedEmotionElem.textContent = "Detected Emotion: " + emotion;

    // Show weighted emotion scores below detected emotion
    let weightedEmotionElem = document.getElementById('weightedEmotionScores');
    if (!weightedEmotionElem) {
      weightedEmotionElem = document.createElement('div');
      weightedEmotionElem.id = 'weightedEmotionScores';
      weightedEmotionElem.style.marginTop = '0.5rem';
      weightedEmotionElem.style.fontWeight = '600';
      emotionResultSection.appendChild(weightedEmotionElem);
    }
    if (weightedEmotions && Object.keys(weightedEmotions).length > 0) {
      let scoresText = "Weighted Emotion Scores: ";
      scoresText += Object.entries(weightedEmotions)
        .map(([emotion, score]) => `${emotion}: ${score}`)
        .join(", ");
      weightedEmotionElem.textContent = scoresText;

      // Update or create pie chart
      updatePieChart(weightedEmotions);
    } else {
      weightedEmotionElem.textContent = "";
      clearPieChart();
    }

    emotionTipsElem.textContent = llmResponse ? "" : "No empathetic response available.";
    emotionResultSection.style.display = "block";
    const llmResponseElem = document.getElementById('llmResponse');
    llmResponseElem.value = llmResponse || "";
  }

  let pieChart = null;

  function updatePieChart(weightedEmotions) {
    const ctx = document.getElementById('weightedEmotionChart').getContext('2d');
    const labels = Object.keys(weightedEmotions);
    const data = Object.values(weightedEmotions);

    if (pieChart) {
      pieChart.data.labels = labels;
      pieChart.data.datasets[0].data = data;
      pieChart.update();
    } else {
      pieChart = new Chart(ctx, {
        type: 'pie',
        data: {
          labels: labels,
          datasets: [{
            data: data,
            backgroundColor: [
              '#4caf50', // green
              '#f44336', // red
              '#ff9800', // orange
              '#2196f3', // blue
              '#9c27b0', // purple
              '#ffeb3b'  // yellow
            ],
            borderWidth: 1
          }]
        },
        options: {
          responsive: true,
          plugins: {
            legend: {
              position: 'bottom',
              labels: {
                color: '#fff'
              }
            }
          }
        }
      });
    }
  }

  function clearPieChart() {
    if (pieChart) {
      pieChart.destroy();
      pieChart = null;
    }
  }

  function resetUI() {
    transcriptArea.value = "";
    detectedEmotionElem.textContent = "";
    emotionTipsElem.textContent = "";
    emotionResultSection.style.display = "none";
    warningMsg.style.display = "none";
  }

  async function startRecording() {
    resetUI();
    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
      warningMsg.textContent = "MediaDevices API or getUserMedia not supported in this browser.";
      warningMsg.style.display = "block";
      return;
    }
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      // Use 'audio/webm' mimeType for better compatibility
      mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });
      audioChunks = [];
      mediaRecorder.ondataavailable = event => {
        if (event.data.size > 0) {
          audioChunks.push(event.data);
        }
      };
      mediaRecorder.onstop = async () => {
        // Convert recorded audio chunks to WAV Blob before sending
        const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
        console.log("Recorded audio blob size:", audioBlob.size, "type:", audioBlob.type);

        // Convert webm Blob to WAV Blob using Web Audio API
        const arrayBuffer = await audioBlob.arrayBuffer();
        const audioContext = new (window.AudioContext || window.webkitAudioContext)();
        const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

        function encodeWAV(audioBuffer) {
          const numChannels = audioBuffer.numberOfChannels;
          const sampleRate = audioBuffer.sampleRate;
          const format = 1; // PCM
          const bitDepth = 16;

          let samples;
          if (numChannels === 2) {
            const channelData0 = audioBuffer.getChannelData(0);
            const channelData1 = audioBuffer.getChannelData(1);
            samples = interleave(channelData0, channelData1);
          } else {
            samples = audioBuffer.getChannelData(0);
          }

          const buffer = new ArrayBuffer(44 + samples.length * 2);
          const view = new DataView(buffer);

          /* RIFF identifier */
          writeString(view, 0, 'RIFF');
          /* file length */
          view.setUint32(4, 36 + samples.length * 2, true);
          /* RIFF type */
          writeString(view, 8, 'WAVE');
          /* format chunk identifier */
          writeString(view, 12, 'fmt ');
          /* format chunk length */
          view.setUint32(16, 16, true);
          /* sample format (raw) */
          view.setUint16(20, format, true);
          /* channel count */
          view.setUint16(22, numChannels, true);
          /* sample rate */
          view.setUint32(24, sampleRate, true);
          /* byte rate (sample rate * block align) */
          view.setUint32(28, sampleRate * numChannels * bitDepth / 8, true);
          /* block align (channel count * bytes per sample) */
          view.setUint16(32, numChannels * bitDepth / 8, true);
          /* bits per sample */
          view.setUint16(34, bitDepth, true);
          /* data chunk identifier */
          writeString(view, 36, 'data');
          /* data chunk length */
          view.setUint32(40, samples.length * 2, true);

          // Write the PCM samples
          floatTo16BitPCM(view, 44, samples);

          return new Blob([view], { type: 'audio/wav' });
        }

        function writeString(view, offset, string) {
          for (let i = 0; i < string.length; i++) {
            view.setUint8(offset + i, string.charCodeAt(i));
          }
        }

        function floatTo16BitPCM(output, offset, input) {
          for (let i = 0; i < input.length; i++, offset += 2) {
            let s = Math.max(-1, Math.min(1, input[i]));
            s = s < 0 ? s * 0x8000 : s * 0x7FFF;
            output.setInt16(offset, s, true);
          }
        }

        function interleave(inputL, inputR) {
          const length = inputL.length + inputR.length;
          const result = new Float32Array(length);

          let index = 0;
          let inputIndex = 0;

          while (index < length) {
            result[index++] = inputL[inputIndex];
            result[index++] = inputR[inputIndex];
            inputIndex++;
          }
          return result;
        }

        const wavBlob = encodeWAV(audioBuffer);

        // Show recorded audio playback on screen
        const audioURL = URL.createObjectURL(wavBlob);
        let existingAudio = document.getElementById('recordedAudioPlayback');
        if (!existingAudio) {
          existingAudio = document.createElement('audio');
          existingAudio.id = 'recordedAudioPlayback';
          existingAudio.controls = true;
          existingAudio.style.marginBottom = '1rem';
          recordButton.parentNode.insertBefore(existingAudio, recordButton.nextSibling);
        }
        existingAudio.src = audioURL;
        existingAudio.load();
        existingAudio.play();

        // Send WAV Blob to backend
        const formData = new FormData();
        formData.append('audio', wavBlob, 'recorded_audio.wav');
        formData.append('language', document.getElementById('languageSelect').value);
        try {
          const response = await fetch('/upload-audio', {
            method: 'POST',
            body: formData
          });
          if (!response.ok) {
            throw new Error('Network response was not ok');
          }
          const data = await response.json();
          if (data.error) {
            warningMsg.textContent = data.error;
            warningMsg.style.display = 'block';
            return;
          }
          transcriptArea.value = data.transcription || '';
          transcriptArea.readOnly = true;
          transcriptArea.placeholder = "Transcription from recorded audio";
          displayEmotion(data.emotion || 'neutral', data.llm_response || '', data.weighted_emotions || {});
          warningMsg.style.display = 'none';
        } catch (error) {
          warningMsg.textContent = `Error processing recorded audio: ${error.message}`;
          warningMsg.style.display = 'block';
        }
      };
      mediaRecorder.start();
      recordButton.textContent = "Recording...";
      recordButton.classList.add("recording");
      isRecording = true;
    } catch (err) {
      warningMsg.textContent = "Error accessing microphone: " + err.message;
      warningMsg.style.display = "block";
    }
  }

  function stopRecording() {
    if (mediaRecorder && mediaRecorder.state !== "inactive") {
      mediaRecorder.stop();
      recordButton.textContent = "Start Recording";
      recordButton.classList.remove("recording");
      isRecording = false;
    }
  }

  recordButton.addEventListener('click', () => {
    if (!isRecording) {
      startRecording();
    } else {
      stopRecording();
    }
  });

  // Handle audio file upload
  audioUpload.addEventListener('change', async (e) => {
    resetUI();
    const file = e.target.files[0];
    if (!file) return;

    // Upload audio file to backend for processing
    const formData = new FormData();
    formData.append('audio', file);
    formData.append('language', document.getElementById('languageSelect').value);

    try {
      const response = await fetch('/upload-audio', {
        method: 'POST',
        body: formData
      });
      if (!response.ok) {
        throw new Error('Network response was not ok');
      }
      const data = await response.json();
      if (data.error) {
        warningMsg.textContent = data.error;
        warningMsg.style.display = 'block';
        return;
      }
      transcriptArea.value = data.transcription || '';
      transcriptArea.readOnly = true;
      transcriptArea.placeholder = "Transcription from uploaded audio";

      // Defensive check and parse weighted_emotions if string
      let weightedEmotions = data.weighted_emotions || {};
      if (typeof weightedEmotions === 'string') {
        try {
          weightedEmotions = JSON.parse(weightedEmotions);
        } catch (e) {
          weightedEmotions = {};
        }
      }

      displayEmotion(data.emotion || 'neutral', data.llm_response || '', weightedEmotions);
      warningMsg.style.display = 'none';

      // Clear file input value to allow re-uploading the same file if needed
      audioUpload.value = '';
    } catch (error) {
      warningMsg.innerHTML = `Error processing audio file: ${error.message}<br>
        You can use the <strong>Start Recording</strong> button for live speech-to-text transcription.<br>
        Alternatively, you can transcribe your audio by external tools and paste text into the box below for emotion analysis.`;
      warningMsg.style.display = 'block';

      // Allow user to paste transcription as fallback
      transcriptArea.readOnly = false;
      transcriptArea.placeholder = "Paste your transcription here...";
      transcriptArea.value = "";

      transcriptArea.addEventListener('input', onTextInputForEmotion);

      function onTextInputForEmotion() {
        const text = transcriptArea.value.trim();
        if (text.length > 0) {
          const dominantEmotion = sentiment.analyze(text);
          displayEmotion(dominantEmotion);
        } else {
          emotionResultSection.style.display = "none";
        }
      }
    }
  });
</script>
</body>
</html>
